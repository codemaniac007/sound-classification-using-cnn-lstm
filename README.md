
# ğŸ”Š Environmental Sound Classification using CNN-LSTM

This project implements an **Environmental Sound Classification (ESC)** system using a hybrid **Convolutional Neural Network (CNN)** and **Long Short-Term Memory (LSTM)** model. It is trained and evaluated on the **UrbanSound8K** dataset, which contains real-world urban sound samples labeled into 10 distinct categories.

---

## ğŸ“ Dataset: UrbanSound8K

- ğŸµ **Classes** (10):  
  - `air_conditioner`, `car_horn`, `children_playing`, `dog_bark`, `drilling`,  
    `engine_idling`, `gun_shot`, `jackhammer`, `siren`, `street_music`

- ğŸ—‚ **Total Samples**: 8,732 audio clips  
- â± **Sampling Rate**: 22,050 Hz  
- ğŸŒ [Download Dataset](https://urbansounddataset.weebly.com/urbansound8k.html)

---

## ğŸ§  Model Architecture

The architecture combines CNNâ€™s spatial feature extraction with LSTMâ€™s sequential modeling power.

```text
Input â†’ Mel-Spectrogram â†’ CNN Layers â†’ Reshape â†’ LSTM Layers â†’ Dense Layers â†’ Output (Softmax)
````

### ğŸ“Œ Components:

* **Mel Spectrogram**: 128 Mel bands per clip.
* **CNN Layers**: Convolution + MaxPooling + Dropout.
* **LSTM Layer**: Processes time-series features extracted by CNN.
* **Dense Layers**: Classifier with softmax activation.

---

## ğŸ›  Features & Techniques

* ğŸ§ Mel-spectrograms as input features.
* ğŸ§¼ Preprocessing: Audio resampling, mono-channel conversion, normalization.
* ğŸ” K-fold cross-validation using fold metadata in UrbanSound8K.
* ğŸ“Š Evaluation: Accuracy, Confusion Matrix, Classification Report.

---

## ğŸ’» Requirements

Install dependencies using pip:

```bash
pip install tensorflow librosa numpy matplotlib scikit-learn
```

---

## ğŸš€ How to Run

1. **Clone this repository**:

   ```bash
   git clone https://github.com/yourusername/esc-cnn-lstm.git
   cd esc-cnn-lstm
   ```

2. **Download UrbanSound8K Dataset**:

   * Extract it into a folder named `UrbanSound8K` in the project root.
   * Update the dataset path in the notebook if necessary.

3. **Run the Notebook**:

   * Open `urbansound8k-cnn-lstm-submission.ipynb` using Jupyter Notebook or Google Colab.
   * Execute all cells sequentially.

---

## ğŸ“Š Results

| Metric                     | Score  |
| -------------------------- | ------ |
| âœ… **Train Accuracy**       | 0.9928 |
| ğŸ§ª **Validation Accuracy** | 0.8725 |
| ğŸ§¾ **Test Accuracy**       | 0.9332 |

* ğŸ“‰ Confusion Matrix:

  * Evaluates class-wise performance.
* ğŸ“ƒ Classification Report:

  * Contains precision, recall, and F1-score for each class.

---

## ğŸ“Œ Future Improvements

* ğŸ› Add data augmentation (e.g., time-stretching, pitch-shifting).
* ğŸ§  Integrate attention or Transformer layers.
* ğŸ“¦ Experiment with more advanced models (CRNN, ResNet).
* ğŸ§ª Hyperparameter optimization using Optuna/KerasTuner.

---

## ğŸ“š References

* ğŸ“„ [UrbanSound8K Dataset](https://urbansounddataset.weebly.com/urbansound8k.html)
* ğŸ§ [Librosa Documentation](https://librosa.org/)
* ğŸ”§ [TensorFlow Documentation](https://www.tensorflow.org/api_docs)
* ğŸ“˜ [Keras Docs](https://keras.io/)

```
```
